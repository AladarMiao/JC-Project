{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2241810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.df = pd.read_csv(self.filename)\n",
    "    \n",
    "    def drop_duplicates(self):\n",
    "        remove_duplicates = input(\"Do you want to remove duplicates? Input T or F \") == \"T\"\n",
    "        if remove_duplicates:\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    def impute_missing(self, method='mean'):\n",
    "        impute_values = input(\"Do you want to impute values? Input T or F \") == \"T\"\n",
    "        if impute_values:\n",
    "            impute_strategy = input(\"How do you want to impute your values? e.g. \\\"median\\\", \\\"0\\\" \" )\n",
    "            imputer = SimpleImputer()\n",
    "            if impute_strategy.isdigit():\n",
    "                imputer = SimpleImputer(strategy='constant', fill_value=int(impute_strategy))\n",
    "            else:\n",
    "                imputer = SimpleImputer(strategy = impute_strategy)\n",
    "            self.df = pd.DataFrame(imputer.fit_transform(self.df), columns=self.df.columns)\n",
    "            \n",
    "    def display_table(self):\n",
    "        display(self.df)\n",
    "        to_csv = input(\"This is your new data file -- do you want to download it? Input T or F \") == \"T\"\n",
    "        if to_csv:\n",
    "            # Get the current timestamp\n",
    "            timestamp = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "            # Define the output filename\n",
    "            # Split the filename into basename and extension\n",
    "            basename, extension = os.path.splitext(self.filename)\n",
    "            download_filename = f'{basename}_{timestamp}{extension}'\n",
    "\n",
    "            # Download the dataframe to the output file\n",
    "            self.df.to_csv(download_filename, index=False)\n",
    "            \n",
    "    def return_data(self):\n",
    "        return self.df.iloc[:, :-1].values\n",
    "        \n",
    "    def return_labels(self):\n",
    "        return self.df.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09d8708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(self, data):\n",
    "        self.model = None\n",
    "        self.data = data\n",
    "        self.pc_to_plot = 2\n",
    "        \n",
    "        #used to calculate different scores, if needed\n",
    "        self.labels = None\n",
    "    \n",
    "    def f1_score(self, labels_true):\n",
    "        return f1_score(labels_true, self.labels)\n",
    "    \n",
    "    def recall_score(self, labels_true):\n",
    "        return recall_score(labels_true, self.labels)\n",
    "    \n",
    "    def precision_score(self, labels_true):\n",
    "        return precision_score(labels_true, self.labels)\n",
    "    \n",
    "    def silhouette_score(self, data):\n",
    "        return silhouette_score(data, self.labels)\n",
    "    \n",
    "    def cluster(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "class KMeansClustering(Clustering):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def elbow_method(self):\n",
    "        # Calculate the within-cluster sum of squares (WCSS) for each k value\n",
    "        wcss = []\n",
    "        \n",
    "        # Define range of k values to test\n",
    "        k_values = range(1, 10)\n",
    "        \n",
    "        for k in k_values:\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            kmeans.fit(self.data)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "\n",
    "        # Plot the elbow curve\n",
    "        plt.plot(k_values, wcss, 'bx-')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('WCSS')\n",
    "        plt.title('Elbow Method')\n",
    "\n",
    "        # Determine the optimal number of clusters\n",
    "        diffs = np.diff(wcss)\n",
    "        diff_ratios = diffs[1:] / diffs[:-1]\n",
    "        optimal_k = k_values[np.argmin(diff_ratios) + 1]\n",
    "\n",
    "        # Display the optimal number of clusters\n",
    "        plt.axvline(x=optimal_k, linestyle='--', color='r', label=f'Optimal k={optimal_k}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "        \n",
    "        return optimal_k\n",
    "    \n",
    "    def cluster(self, n_clusters=None):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        if n_clusters is None:\n",
    "            n_clusters = self.elbow_method()\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        self.model = kmeans\n",
    "        self.labels = kmeans.fit_predict(self.data)\n",
    "        \n",
    "        #plot the clusters\n",
    "        self.plot_clusters(kmeans)\n",
    "    \n",
    "    def plot_clusters(self, kmeans):\n",
    "        # Fit PCA to the data\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(self.data)\n",
    "\n",
    "        # Plot the clustered data on the two principal components\n",
    "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "\n",
    "        # Plot the centroids on the two principal components\n",
    "        centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], marker='x', s=100, linewidths=3, color='r')\n",
    "\n",
    "        # Set plot title and axis labels\n",
    "        plt.title('K-means Clustering on PCA')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        \n",
    "        # Remove values on the x and y axes\n",
    "        plt.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "class DBSCANClustering(Clustering):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def cluster(self, eps=None, min_samples=None):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        if eps is None:\n",
    "#             eps = np.sqrt(self.data.shape[1])\n",
    "            eps=5000\n",
    "        if min_samples is None:\n",
    "            min_samples = 2 * self.data.shape[1]\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        self.model = dbscan\n",
    "        self.labels = dbscan.fit_predict(self.data)\n",
    "        \n",
    "        #plot the clusters\n",
    "        self.plot_clusters(dbscan)\n",
    "    \n",
    "    def plot_clusters(self, dbscan):\n",
    "        # Fit PCA to the data\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(self.data)\n",
    "        \n",
    "        # Get the cluster labels and the number of clusters\n",
    "        labels = dbscan.labels_\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        # Plot the data points colored by cluster\n",
    "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=50, cmap='viridis')\n",
    "        plt.title(f'DBSCAN Clustering on PCA (Number of Clusters: {n_clusters})')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        \n",
    "        # Remove values on the x and y axes\n",
    "        plt.tick_params(labelbottom=False, labelleft=False)\n",
    "        \n",
    "        #Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class AgglomerativeClusteringAlgorithm(Clustering):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    # The linkage parameter specifies the method used to compute the distance between clusters. \n",
    "    # It determines the way clusters are merged during the hierarchical clustering process.\n",
    "    def cluster(self, n_clusters=2, linkage='ward'):\n",
    "        self.model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        self.labels = model.fit_predict(self.data)\n",
    "        \n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "class MeanShiftClustering(Clustering):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def cluster(self):\n",
    "        self.model = MeanShift()\n",
    "        self.labels = self.model.fit_predict(self.data)\n",
    "        self.centers = self.model.cluster_centers_\n",
    "        \n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class IsolationForestClustering(Clustering):\n",
    "    def __init__(self, data, contamination=0.05):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def cluster(self):\n",
    "        self.model = IsolationForest(contamination=self.contamination)\n",
    "        self.model.fit(self.data)\n",
    "        self.labels = self.clf.predict(self.data)\n",
    "        \n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "class BGMMClustering(Clustering):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def cluster(self, n_components=None):\n",
    "        if n_components is None:\n",
    "            n_components = 2\n",
    "        self.model = BayesianGaussianMixture(n_components=n_components)\n",
    "        self.model.fit(self.data)\n",
    "        self.labels = self.model.predict(self.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7962c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "class AnomalyDetection:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def detect(self, new_data):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class AutoEncoder(AnomalyDetection):\n",
    "    def __init__(self, data, encoding_dim):\n",
    "        super().__init__(data)\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self):\n",
    "        input_data = Input(shape=(self.data.shape[1],))\n",
    "        #should these parameters be defined by the user?\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_data)\n",
    "        decoded = Dense(self.data.shape[1], activation='sigmoid')(encoded)\n",
    "        self.model = Model(input_data, decoded)\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "        self.model.fit(self.data, self.data, epochs=50, batch_size=32)\n",
    "    \n",
    "    def detect(self, new_data):\n",
    "        reconstructions = self.model.predict(new_data)\n",
    "        mse = np.mean(np.power(new_data - reconstructions, 2), axis=1)\n",
    "        return mse\n",
    "    \n",
    "#maybe add GAN-based anomaly detection as mentioned in the paper \"Explainable AI Approach for MOD Outliers in FAB to MOD Process\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ef383b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    \n",
    "    @staticmethod\n",
    "    def getClusteringClass(user_input_clustering_algorithm, data):\n",
    "        # create a dictionary that maps user input to the corresponding class\n",
    "        clustering_class_dict = {\n",
    "            \"kmeans\": KMeansClustering(data),\n",
    "            \"dbscan\": DBSCANClustering(data),\n",
    "            \"agglomerative\": AgglomerativeClusteringAlgorithm(data),\n",
    "            \"meanshift\": MeanShiftClustering(data),\n",
    "            \"isolation forest\": IsolationForestClustering(data),\n",
    "            \"bgmm\": BGMMClustering(data)\n",
    "        }\n",
    "\n",
    "        return clustering_class_dict[user_input_clustering_algorithm]\n",
    "    \n",
    "class App:\n",
    "    def __init__(self, filename):\n",
    "        self.preprocessor = DataPreprocessor(filename)\n",
    "        self.clustering = None\n",
    "        \n",
    "    def start(self):\n",
    "        \n",
    "        # Ask the user for preprocessing options\n",
    "        self.preprocessor.drop_duplicates()\n",
    "        self.preprocessor.impute_missing()\n",
    "\n",
    "        # Show the user how the new table looks, and ask the user if he/she wants to download it\n",
    "        self.preprocessor.display_table()\n",
    "\n",
    "        data = self.preprocessor.return_data()\n",
    "\n",
    "        # Ask the user for clustering options\n",
    "        clustering_algorithm = input(\"\"\"Which clustering algorithm do you wish to use? We currently support the following:\n",
    "                                        kmeans, dbscan, meanshift, agglomerative, isolation forest, bgmm \"\"\")\n",
    "        \n",
    "        self.clustering = Util.getClusteringClass(clustering_algorithm, data)\n",
    "\n",
    "        # Perform clustering\n",
    "        self.clustering.cluster()\n",
    "        \n",
    "        # Measure performance\n",
    "        silhouette_score = self.clustering.silhouette_score(data)\n",
    "\n",
    "        # Print the score\n",
    "        print(\"Silhouette score:\", silhouette_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9878cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to remove duplicates? Input T or F T\n",
      "Do you want to impute values? Input T or F T\n",
      "How do you want to impute your values? e.g. \"median\", \"0\" mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bright_sum</th>\n",
       "      <th>bright_size</th>\n",
       "      <th>bright_density</th>\n",
       "      <th>dark_sum</th>\n",
       "      <th>dark_size</th>\n",
       "      <th>dark_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>104.761905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.861505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>112.729167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.861505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4041.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>109.216216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.861505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16782.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>112.630872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.861505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>31296.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>117.213483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.861505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>1934.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.969190</td>\n",
       "      <td>46759.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>127.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>1935.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.969190</td>\n",
       "      <td>26529.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>130.684729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1936.0</td>\n",
       "      <td>2692.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>107.680000</td>\n",
       "      <td>120569.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>125.986416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>1937.0</td>\n",
       "      <td>4334.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>105.707317</td>\n",
       "      <td>84513.0</td>\n",
       "      <td>643.0</td>\n",
       "      <td>131.435459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>1938.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.969190</td>\n",
       "      <td>631.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>126.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1939 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  bright_sum  bright_size  bright_density  dark_sum   \n",
       "0            0.0      2200.0         21.0      104.761905       0.0  \\\n",
       "1            1.0      5411.0         48.0      112.729167       0.0   \n",
       "2            2.0      4041.0         37.0      109.216216       0.0   \n",
       "3            3.0     16782.0        149.0      112.630872       0.0   \n",
       "4            4.0     31296.0        267.0      117.213483       0.0   \n",
       "...          ...         ...          ...             ...       ...   \n",
       "1934      1934.0         0.0          0.0      116.969190   46759.0   \n",
       "1935      1935.0         0.0          0.0      116.969190   26529.0   \n",
       "1936      1936.0      2692.0         25.0      107.680000  120569.0   \n",
       "1937      1937.0      4334.0         41.0      105.707317   84513.0   \n",
       "1938      1938.0         0.0          0.0      116.969190     631.0   \n",
       "\n",
       "      dark_size  dark_density  \n",
       "0           0.0    137.861505  \n",
       "1           0.0    137.861505  \n",
       "2           0.0    137.861505  \n",
       "3           0.0    137.861505  \n",
       "4           0.0    137.861505  \n",
       "...         ...           ...  \n",
       "1934      368.0    127.062500  \n",
       "1935      203.0    130.684729  \n",
       "1936      957.0    125.986416  \n",
       "1937      643.0    131.435459  \n",
       "1938        5.0    126.200000  \n",
       "\n",
       "[1939 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your new data file -- do you want to download it? Input T or F F\n",
      "Which clustering algorithm do you wish to use? We currently support the following:\n",
      "                                        kmeans, dbscan, meanshift, agglomerative, isolation forest, bgmmbgmm\n",
      "Silhouette score: 0.6492706469630607\n"
     ]
    }
   ],
   "source": [
    "app = App(\"mura_data.csv\")\n",
    "app.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a7a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
